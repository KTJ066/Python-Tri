{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99848cad-c086-4140-b200-ce7811c81d76",
   "metadata": {},
   "source": [
    "토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877ed18a-8027-472f-81f4-269a3ade9ede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonghyunkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bcf82d-cad6-4bb8-b570-164382e112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conanob = \"We are BaDa. We are BusinessAnalytics studygroup in Korea University Business school. Our major curriculum is studying Analytics tool, and Case study of Analyzing Business situations. If you would like to join us, you may contact us by our website.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd34b994-dc8b-4e13-b8c4-b314c67ac854",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are BaDa.', 'We are BusinessAnalytics studygroup in Korea University Business school.', 'Our major curriculum is studying Analytics tool, and Case study of Analyzing Business situations.', 'If you would like to join us, you may contact us by our website.']\n",
      "['We', 'are', 'BaDa', '.', 'We', 'are', 'BusinessAnalytics', 'studygroup', 'in', 'Korea', 'University', 'Business', 'school', '.', 'Our', 'major', 'curriculum', 'is', 'studying', 'Analytics', 'tool', ',', 'and', 'Case', 'study', 'of', 'Analyzing', 'Business', 'situations', '.', 'If', 'you', 'would', 'like', 'to', 'join', 'us', ',', 'you', 'may', 'contact', 'us', 'by', 'our', 'website', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = sent_tokenize(conanob)\n",
    "print(sentence)\n",
    "word = word_tokenize(conanob)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1823776e-9c9e-49b2-a9c2-f6e9d74034a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3m/s0ysqyzs3fb_mcgvm4wjzt3w0000gn/T/ipykernel_7812/2627233961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \"\"\"\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \"\"\"\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \"\"\"\n\u001b[1;32m   1364\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "word2 = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136fefa9-8c0e-463a-98cd-739c333d6c6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We', 'are', 'BaDa', '.'],\n",
       " ['We',\n",
       "  'are',\n",
       "  'BusinessAnalytics',\n",
       "  'studygroup',\n",
       "  'in',\n",
       "  'Korea',\n",
       "  'University',\n",
       "  'Business',\n",
       "  'school',\n",
       "  '.'],\n",
       " ['Our',\n",
       "  'major',\n",
       "  'curriculum',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'Analytics',\n",
       "  'tool',\n",
       "  ',',\n",
       "  'and',\n",
       "  'Case',\n",
       "  'study',\n",
       "  'of',\n",
       "  'Analyzing',\n",
       "  'Business',\n",
       "  'situations',\n",
       "  '.'],\n",
       " ['If',\n",
       "  'you',\n",
       "  'would',\n",
       "  'like',\n",
       "  'to',\n",
       "  'join',\n",
       "  'us',\n",
       "  ',',\n",
       "  'you',\n",
       "  'may',\n",
       "  'contact',\n",
       "  'us',\n",
       "  'by',\n",
       "  'our',\n",
       "  'website',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3 = []\n",
    "for s in sentence:\n",
    "    w = word_tokenize(s)\n",
    "    word3.append(w)\n",
    "    \n",
    "word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d287d5-1588-4a52-8aaf-9f5958379c6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'BaDa',\n",
       " 'We',\n",
       " 'are',\n",
       " 'BusinessAnalytics',\n",
       " 'studygroup',\n",
       " 'in',\n",
       " 'Korea',\n",
       " 'University',\n",
       " 'Business',\n",
       " 'school',\n",
       " 'Our',\n",
       " 'major',\n",
       " 'curriculum',\n",
       " 'is',\n",
       " 'studying',\n",
       " 'Analytics',\n",
       " 'tool',\n",
       " 'and',\n",
       " 'Case',\n",
       " 'study',\n",
       " 'of',\n",
       " 'Analyzing',\n",
       " 'Business',\n",
       " 'situations',\n",
       " 'If',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'join',\n",
       " 'us',\n",
       " 'you',\n",
       " 'may',\n",
       " 'contact',\n",
       " 'us',\n",
       " 'by',\n",
       " 'our',\n",
       " 'website']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word4 = tokenizer.tokenize(conanob)\n",
    "word4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e046138-ef2e-4fa3-885b-40eb2c86b035",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We', 'are', 'BaDa'],\n",
       " ['We',\n",
       "  'are',\n",
       "  'BusinessAnalytics',\n",
       "  'studygroup',\n",
       "  'in',\n",
       "  'Korea',\n",
       "  'University',\n",
       "  'Business',\n",
       "  'school'],\n",
       " ['Our',\n",
       "  'major',\n",
       "  'curriculum',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'Analytics',\n",
       "  'tool',\n",
       "  'and',\n",
       "  'Case',\n",
       "  'study',\n",
       "  'of',\n",
       "  'Analyzing',\n",
       "  'Business',\n",
       "  'situations'],\n",
       " ['If',\n",
       "  'you',\n",
       "  'would',\n",
       "  'like',\n",
       "  'to',\n",
       "  'join',\n",
       "  'us',\n",
       "  'you',\n",
       "  'may',\n",
       "  'contact',\n",
       "  'us',\n",
       "  'by',\n",
       "  'our',\n",
       "  'website']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word5 = []\n",
    "for se in sentence:\n",
    "    wo = tokenizer.tokenize(se)\n",
    "    word5.append(wo)\n",
    "    \n",
    "word5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82546f-2ac6-4062-b8d2-bb9ae2209f49",
   "metadata": {},
   "source": [
    "불용어 배제(Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c061fcce-9a55-4699-bb7b-6bca71a28328",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'yourself', 'already', 'beside', 'thick', 'the', 'besides', 'therefore', 'keep', 'had', 'nothing', 'same', 'please', 'call', 'become', 'eight', 'of', 'enough', 'thereby', 'this', 'everything', 'about', 'somewhere', 'over', 'eleven', 'wherever', 'formerly', 'inc', 'their', 'another', 'hers', 'nor', 'meanwhile', 'part', 'etc', 'than', 'ltd', 'be', 'hereafter', 'though', 'out', 'thus', 'very', 'any', 'too', 'nine', 'off', 'what', 'describe', 'and', 'nobody', 'mill', 'sixty', 'bottom', 'more', 'these', 'you', 'found', 'top', 'namely', 'whenever', 'but', 'fifteen', 'which', 'at', 'anyway', 'front', 'such', 'we', 'will', 'others', 'often', 'for', 'nevertheless', 'he', 'its', 'behind', 'cry', 'un', 're', 'side', 'why', 'other', 'almost', 'below', 'seeming', 'when', 'one', 'three', 'co', 'always', 'within', 'that', 'twelve', 'until', 'six', 'con', 'before', 'put', 'through', 'seemed', 'ie', 'with', 'hasnt', 'i', 'show', 'fill', 'to', 'else', 'wherein', 'see', 'take', 'themselves', 'whither', 'hence', 'otherwise', 'because', 'between', 'only', 'toward', 'interest', 'twenty', 'thru', 'seems', 'detail', 'each', 'around', 'sometimes', 'whose', 'anywhere', 'hereby', 'his', 'in', 'former', 'except', 'elsewhere', 'many', 'sincere', 'four', 'anyone', 'much', 'itself', 'since', 'no', 'whereby', 'has', 'empty', 'latter', 'due', 'upon', 'well', 'onto', 'would', 'amoungst', 'an', 'must', 'thin', 'hereupon', 'via', 'after', 'there', 'therein', 'somehow', 'ever', 'find', 'cant', 'once', 'rather', 'am', 'name', 'above', 'afterwards', 'together', 'being', 'per', 'whole', 'where', 'yours', 'although', 'herself', 'she', 'have', 'were', 'hundred', 'our', 'down', 'sometime', 'go', 'made', 'mine', 'then', 'most', 'him', 'a', 'couldnt', 'your', 'de', 'her', 'least', 'own', 'less', 'indeed', 'full', 'us', 'get', 'without', 'whatever', 'so', 'latterly', 'ourselves', 'thereupon', 'can', 'could', 'against', 'however', 'should', 'either', 'if', 'or', 'beyond', 'two', 'now', 'further', 'move', 'been', 'first', 'may', 'me', 'fire', 'under', 'on', 'became', 'serious', 'next', 'moreover', 'across', 'forty', 'do', 'few', 'some', 'towards', 'is', 'alone', 'my', 'here', 'ours', 'both', 'into', 'herein', 'throughout', 'never', 'yourselves', 'whom', 'myself', 'everyone', 'beforehand', 'nowhere', 'still', 'even', 'thereafter', 'are', 'whence', 'ten', 'how', 'as', 'again', 'cannot', 'none', 'back', 'eg', 'done', 'anyhow', 'himself', 'whoever', 'every', 'them', 'also', 'whether', 'not', 'they', 'while', 'noone', 'along', 'give', 'seem', 'amongst', 'whereas', 'who', 'amount', 'something', 'bill', 'becoming', 'last', 'neither', 'third', 'everywhere', 'fifty', 'becomes', 'up', 'all', 'during', 'thence', 'might', 'whereafter', 'several', 'by', 'system', 'whereupon', 'five', 'was', 'someone', 'from', 'among', 'it', 'those', 'anything', 'mostly', 'yet', 'perhaps'})\n"
     ]
    }
   ],
   "source": [
    "##불용어 배제(Stopwords)\n",
    "from sklearn.feature_extraction import text\n",
    "sklearn_stopwords = text.ENGLISH_STOP_WORDS\n",
    "print(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b2dea8-3a16-4bfc-a7b1-3f7004a2b45c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'BaDa',\n",
       " 'We',\n",
       " 'BusinessAnalytics',\n",
       " 'studygroup',\n",
       " 'Korea',\n",
       " 'University',\n",
       " 'Business',\n",
       " 'school',\n",
       " 'Our',\n",
       " 'major',\n",
       " 'curriculum',\n",
       " 'studying',\n",
       " 'Analytics',\n",
       " 'tool',\n",
       " 'Case',\n",
       " 'study',\n",
       " 'Analyzing',\n",
       " 'Business',\n",
       " 'situations',\n",
       " 'If',\n",
       " 'like',\n",
       " 'join',\n",
       " 'contact',\n",
       " 'website']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Bag of words 형태로 뽑힌 것들 중 가장 깔끔하게 구두점 없이 형성된 것이 word4니까 word4 사용\n",
    "useful = []\n",
    "for word in word4:\n",
    "    if not word in sklearn_stopwords:\n",
    "        useful.append(word)\n",
    "useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7226f-c197-4444-88ac-113b47fa5306",
   "metadata": {},
   "source": [
    "어간추출(Stemming) &  표제어추출(Lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5727cb-e215-47aa-bc21-aef6d8491ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 어간추출(Stemmeing)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7218ce-3187-4eed-a18b-5fc7ad959df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 표제어추출(Lemmatizing)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae3ed8e-d5b6-4c48-9b3a-b5c2df912dff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_result ['fli', 'fli', 'flew']\n",
      "ls_result ['fly', 'fly', 'flew']\n",
      "ps_result ['organ', 'organ']\n",
      "ls_result ['org', 'org']\n"
     ]
    }
   ],
   "source": [
    "## Stemming tool 간의 차이를 보자\n",
    "bow1 = [\"fly\", \"flying\", \"flew\"]\n",
    "print(\"ps_result\", [ps.stem(i) for i in bow1])\n",
    "print(\"ls_result\", [ls.stem(w) for w in bow1])\n",
    "bow2 = [\"organs\", \"organization\"]\n",
    "print(\"ps_result\", [ps.stem(i) for i in bow2])\n",
    "print(\"ls_result\", [ls.stem(w) for w in bow2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c3baa4-ecd7-4542-9ce8-da59daf8d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위와 같은 결과 때문에, stemming tool을 사용한 후에는 어간추출이 잘 이루어졌는지 잘 확인해보는 것이 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea1c041-3526-4261-b806-92b3f49be519",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_result ['fly', 'flying', 'flew']\n"
     ]
    }
   ],
   "source": [
    "##Lemmatizing tool?\n",
    "print(\"lm_result\", [lm.lemmatize(w) for w in bow1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb8752a-39f4-4175-9583-bf5ca3b60055",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tagging [('fly', 'NN'), ('flying', 'VBG'), ('flew', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "## pos 정보를 입력해주어야 한다.\n",
    "from nltk.tag import pos_tag\n",
    "pos_l = pos_tag(bow1)\n",
    "print('pos_tagging', pos_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537f1527-6bca-4a9f-a74d-8c735b7a2f09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('flying', 'v') ## 보다 싶이 태깅을 통해 불러온 품사 정보와, 우리가 lemmatizing 시 입력해야 하는 품사의 형태가 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0e62b-6c2f-4908-a8f2-ebc3f59e840d",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d7bd2a-c362-4171-ab83-cc6e55c86cae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analytics',\n",
       " 'Analyzing',\n",
       " 'BaDa',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'BusinessAnalytics',\n",
       " 'Case',\n",
       " 'If',\n",
       " 'Korea',\n",
       " 'Our',\n",
       " 'University',\n",
       " 'We',\n",
       " 'We',\n",
       " 'and',\n",
       " 'are',\n",
       " 'are',\n",
       " 'by',\n",
       " 'contact',\n",
       " 'curriculum',\n",
       " 'in',\n",
       " 'is',\n",
       " 'join',\n",
       " 'like',\n",
       " 'major',\n",
       " 'may',\n",
       " 'of',\n",
       " 'our',\n",
       " 'school',\n",
       " 'situations',\n",
       " 'study',\n",
       " 'studygroup',\n",
       " 'studying',\n",
       " 'to',\n",
       " 'tool',\n",
       " 'us',\n",
       " 'us',\n",
       " 'website',\n",
       " 'would',\n",
       " 'you',\n",
       " 'you']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TF-IDF 직접 해보자\n",
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log # IDF 계산을 위해\n",
    "\n",
    "word4.sort()\n",
    "word4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea2a3b5-5dce-4242-9917-c38f734aa3c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analytics', 'Analyzing', 'BaDa', 'Business', 'BusinessAnalytics', 'Case', 'If', 'Korea', 'Our', 'University', 'We', 'and', 'are', 'by', 'contact', 'curriculum', 'in', 'is', 'join', 'like', 'major', 'may', 'of', 'our', 'school', 'situations', 'study', 'studygroup', 'studying', 'to', 'tool', 'us', 'website', 'would', 'you']\n"
     ]
    }
   ],
   "source": [
    "bow = []\n",
    "for v in word4:\n",
    "    if v not in bow:\n",
    "        bow.append(v)\n",
    "print(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5201c57f-5e9a-4715-8e74-ff49a246faed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 총 문서의 수\n",
    "N = len(sentence) \n",
    "\n",
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for sent in sentence:\n",
    "    df += t in sent\n",
    "  return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6ef596-4798-49ef-a4ae-605d219fc9bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analytics</th>\n",
       "      <th>Analyzing</th>\n",
       "      <th>BaDa</th>\n",
       "      <th>Business</th>\n",
       "      <th>BusinessAnalytics</th>\n",
       "      <th>Case</th>\n",
       "      <th>If</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Our</th>\n",
       "      <th>University</th>\n",
       "      <th>...</th>\n",
       "      <th>situations</th>\n",
       "      <th>study</th>\n",
       "      <th>studygroup</th>\n",
       "      <th>studying</th>\n",
       "      <th>to</th>\n",
       "      <th>tool</th>\n",
       "      <th>us</th>\n",
       "      <th>website</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analytics  Analyzing  BaDa  Business  BusinessAnalytics  Case  If  Korea  \\\n",
       "0          0          0     1         0                  0     0   0      0   \n",
       "1          1          0     0         2                  1     0   0      1   \n",
       "2          1          1     0         1                  0     1   0      0   \n",
       "3          0          0     0         0                  0     0   1      0   \n",
       "\n",
       "   Our  University  ...  situations  study  studygroup  studying  to  tool  \\\n",
       "0    0           0  ...           0      0           0         0   0     0   \n",
       "1    0           1  ...           0      1           1         0   0     0   \n",
       "2    1           0  ...           1      2           0         1   1     1   \n",
       "3    0           0  ...           0      0           0         0   1     0   \n",
       "\n",
       "   us  website  would  you  \n",
       "0   0        0      0    0  \n",
       "1   2        0      0    0  \n",
       "2   1        0      0    0  \n",
       "3   2        1      1    2  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = sentence[i]\n",
    "  for j in range(len(bow)):\n",
    "    t = bow[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = bow)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "511ca55f-d12c-4d50-a6c7-0fe3598cdf19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Analytics</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyzing</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaDa</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessAnalytics</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Our</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curriculum</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situations</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studygroup</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studying</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tool</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>website</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        IDF\n",
       "Analytics          0.287682\n",
       "Analyzing          0.693147\n",
       "BaDa               0.693147\n",
       "Business           0.287682\n",
       "BusinessAnalytics  0.693147\n",
       "Case               0.693147\n",
       "If                 0.693147\n",
       "Korea              0.693147\n",
       "Our                0.693147\n",
       "University         0.693147\n",
       "We                 0.287682\n",
       "and                0.693147\n",
       "are                0.287682\n",
       "by                 0.693147\n",
       "contact            0.693147\n",
       "curriculum         0.693147\n",
       "in                 0.000000\n",
       "is                 0.693147\n",
       "join               0.693147\n",
       "like               0.693147\n",
       "major              0.693147\n",
       "may                0.693147\n",
       "of                 0.693147\n",
       "our                0.693147\n",
       "school             0.693147\n",
       "situations         0.693147\n",
       "study              0.287682\n",
       "studygroup         0.693147\n",
       "studying           0.693147\n",
       "to                 0.287682\n",
       "tool               0.693147\n",
       "us                 0.000000\n",
       "website            0.693147\n",
       "would              0.693147\n",
       "you                0.693147"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(bow)):\n",
    "    t = bow[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=bow, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b686d52-a914-4f29-918c-acc745b9e529",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analytics</th>\n",
       "      <th>Analyzing</th>\n",
       "      <th>BaDa</th>\n",
       "      <th>Business</th>\n",
       "      <th>BusinessAnalytics</th>\n",
       "      <th>Case</th>\n",
       "      <th>If</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Our</th>\n",
       "      <th>University</th>\n",
       "      <th>...</th>\n",
       "      <th>situations</th>\n",
       "      <th>study</th>\n",
       "      <th>studygroup</th>\n",
       "      <th>studying</th>\n",
       "      <th>to</th>\n",
       "      <th>tool</th>\n",
       "      <th>us</th>\n",
       "      <th>website</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analytics  Analyzing      BaDa  Business  BusinessAnalytics      Case  \\\n",
       "0   0.000000   0.000000  0.693147  0.000000           0.000000  0.000000   \n",
       "1   0.287682   0.000000  0.000000  0.575364           0.693147  0.000000   \n",
       "2   0.287682   0.693147  0.000000  0.287682           0.000000  0.693147   \n",
       "3   0.000000   0.000000  0.000000  0.000000           0.000000  0.000000   \n",
       "\n",
       "         If     Korea       Our  University  ...  situations     study  \\\n",
       "0  0.000000  0.000000  0.000000    0.000000  ...    0.000000  0.000000   \n",
       "1  0.000000  0.693147  0.000000    0.693147  ...    0.000000  0.287682   \n",
       "2  0.000000  0.000000  0.693147    0.000000  ...    0.693147  0.575364   \n",
       "3  0.693147  0.000000  0.000000    0.000000  ...    0.000000  0.000000   \n",
       "\n",
       "   studygroup  studying        to      tool   us   website     would       you  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "1    0.693147  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.693147  0.287682  0.693147  0.0  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.287682  0.000000  0.0  0.693147  0.693147  1.386294  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = sentence[i]\n",
    "  for j in range(len(bow)):\n",
    "    t = bow[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = bow)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57f48299-0ad8-495e-9855-8d6b25b02bc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
      " [1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 2 0 1 1 2]]\n",
      "{'we': 30, 'are': 3, 'bada': 4, 'businessanalytics': 6, 'studygroup': 24, 'in': 12, 'korea': 15, 'university': 28, 'business': 5, 'school': 21, 'our': 20, 'major': 17, 'curriculum': 10, 'is': 13, 'studying': 25, 'analytics': 0, 'tool': 27, 'and': 2, 'case': 8, 'study': 23, 'of': 19, 'analyzing': 1, 'situations': 22, 'if': 11, 'you': 33, 'would': 32, 'like': 16, 'to': 26, 'join': 14, 'us': 29, 'may': 18, 'contact': 9, 'by': 7, 'website': 31}\n"
     ]
    }
   ],
   "source": [
    "##Sklearn을 이용한 TF-IDF 툴\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(sentence).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea54f4e4-fc2d-49df-9960-f0b8c44ad955",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.52640543 0.66767854 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52640543 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.28113163 0.         0.28113163\n",
      "  0.35657982 0.         0.         0.         0.         0.\n",
      "  0.35657982 0.         0.         0.35657982 0.         0.\n",
      "  0.         0.         0.         0.35657982 0.         0.\n",
      "  0.35657982 0.         0.         0.         0.35657982 0.\n",
      "  0.28113163 0.         0.         0.        ]\n",
      " [0.2747918  0.2747918  0.2747918  0.         0.         0.21664901\n",
      "  0.         0.         0.2747918  0.         0.2747918  0.\n",
      "  0.         0.2747918  0.         0.         0.         0.2747918\n",
      "  0.         0.2747918  0.21664901 0.         0.2747918  0.2747918\n",
      "  0.         0.2747918  0.         0.2747918  0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23821956 0.         0.23821956 0.         0.23821956\n",
      "  0.         0.         0.23821956 0.         0.23821956 0.\n",
      "  0.23821956 0.         0.18781504 0.         0.         0.\n",
      "  0.         0.         0.23821956 0.         0.         0.47643912\n",
      "  0.         0.23821956 0.23821956 0.47643912]]\n",
      "{'we': 30, 'are': 3, 'bada': 4, 'businessanalytics': 6, 'studygroup': 24, 'in': 12, 'korea': 15, 'university': 28, 'business': 5, 'school': 21, 'our': 20, 'major': 17, 'curriculum': 10, 'is': 13, 'studying': 25, 'analytics': 0, 'tool': 27, 'and': 2, 'case': 8, 'study': 23, 'of': 19, 'analyzing': 1, 'situations': 22, 'if': 11, 'you': 33, 'would': 32, 'like': 16, 'to': 26, 'join': 14, 'us': 29, 'may': 18, 'contact': 9, 'by': 7, 'website': 31}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(sentence)\n",
    "print(tfidfv.transform(sentence).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
